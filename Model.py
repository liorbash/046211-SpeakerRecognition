# -*- coding: utf-8 -*-
"""resnet50_model_(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/183GmqOSNXrJAbWH0FoBkprvoz0VMhb1c
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import torch.backends.cudnn as cudnn
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import copy
import pandas as pd
#import optuna

import DataLoader
from Trainer import Trainer, initialize_model

from google.colab import drive
drive.mount('/content/drive')

# device - cpu or gpu?
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


"""**Freezed model**"""

feature_extract = True # freezing
resnet18_model_freezed, input_size = initialize_model('resnet', num_classes=400, feature_extract=feature_extract, use_pretrained=True)
resnet18_model_freezed = resnet18_model_freezed.to(device)

# Gather the parameters to be optimized/updated in this run. If we are
# fine-tuning we will be updating all parameters. However, if we are
# doing feature extract method, we will only update the parameters
# that we have just initialized, i.e. the parameters with requires_grad
# is True.
params_to_update = resnet18_model_freezed.parameters()
print("Params to learn:")
if feature_extract:
  params_to_update = []
  for name,param in resnet18_model_freezed.named_parameters():
    if param.requires_grad == True:
      params_to_update.append(param)
      print("\t",name)
else:
  for name,param in resnet18_model_freezed.named_parameters():
    if param.requires_grad == True:
      print("\t",name)
    
criterion = nn.CrossEntropyLoss()
# Observe that all parameters are being optimized
#optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)
optimizer_ft = optim.Adam(params_to_update, lr=0.001, betas=(0.9, 0.98), eps=1e-09)
# Decay LR by a factor of 0.1 every 7 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

resnet18_freezed_trainer = Trainer(resnet18_model_freezed, device, input_size, criterion, optimizer_ft, exp_lr_scheduler, batch_size=32, num_epochs=1, num_workers=2, dataset_dir='./drive/MyDrive/VoxCeleb1')

#resnet18_freezed_trainer.train_model('train')

#resnet18_freezed_trainer.evaluate_model()

"""**Fine-tuning model**"""

feature_extract = False # fine-tuning
resnet18_model_fine_tuning, input_size = initialize_model('resnet', num_classes=400, feature_extract=feature_extract, use_pretrained=True)
resnet18_model_fine_tuning = resnet18_model_fine_tuning.to(device)

# Gather the parameters to be optimized/updated in this run. If we are
# fine-tuning we will be updating all parameters. However, if we are
# doing feature extract method, we will only update the parameters
# that we have just initialized, i.e. the parameters with requires_grad
# is True.
params_to_update = resnet18_model_fine_tuning.parameters()
if feature_extract:
  params_to_update = []
  for name,param in resnet18_model_fine_tuning.named_parameters():
    if param.requires_grad == True:
      params_to_update.append(param)

criterion = nn.CrossEntropyLoss()
# Observe that all parameters are being optimized
#optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)
optimizer_ft = optim.Adam(params_to_update, lr=0.001, betas=(0.9, 0.98), eps=1e-09)
# Decay LR by a factor of 0.1 every 7 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

resnet18_model_fine_tuning_trainer = Trainer(resnet18_model_fine_tuning, device, input_size, criterion, optimizer_ft, exp_lr_scheduler, batch_size=32, num_epochs=5, num_workers=2, dataset_dir='./drive/MyDrive/VoxCeleb1')

#resnet18_model_fine_tuning_trainer.train_model()