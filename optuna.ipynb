{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlygGpAwUN-7",
        "outputId": "bbe02235-6587-4f91-c6ba-908d302a2eae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.5/348.5 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.0/81.0 KB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.6/210.6 KB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.1/147.1 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KmLw8Dr01-lh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "05365d08-fa3d-4ac3-ae4c-7a0f571a6d33"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-eb984c1e637f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mModel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresnet18_model_fine_tuning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet18_model_freezed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "#import optuna\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "import torch.nn.functional as F\n",
        "from Model import resnet18_model_fine_tuning, resnet18_model_freezed\n",
        "import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4g6BQzS3Q46"
      },
      "outputs": [],
      "source": [
        "# device - cpu or gpu?\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F90eQ00B2DvR"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "epochs = 10\n",
        "log_interval = 10\n",
        "n_train_examples = batch_size * 30\n",
        "n_valid_examples = batch_size * 10\n",
        "\n",
        "def get_model():\n",
        "  return resnet18_model_fine_tuning\n",
        "\n",
        "def objective(trial):\n",
        "  # Generate the model.\n",
        "  model = get_model() # Already on device\n",
        "  \n",
        "  # Generate the optimizers.\n",
        "  lr = trial.suggest_float(\"lr\", 1e-7, 1e-4, log=True) # log=True, will use log scale to interplolate between lr\n",
        "  optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "  optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "  \n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # Get Data loader\n",
        "  train_dataset = DataLoader.VoxCaleb1MelSpecDataset('./drive/MyDrive/VoxCeleb1', 'train', input_size=224, download=False)\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=2)\n",
        "  valid_dataset = DataLoader.VoxCaleb1MelSpecDataset('./drive/MyDrive/VoxCeleb1', 'val', input_size=224, download=False)\n",
        "  valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, num_workers=2)\n",
        "  # Training of the model.\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "      # Limiting training data for faster epochs.\n",
        "      if batch_idx * batch_size >= n_train_examples:\n",
        "        break\n",
        "\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    \n",
        "    # Validation of the model.\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (data, target) in enumerate(valid_loader):\n",
        "        # Limiting validation data.\n",
        "        if batch_idx * batch_size >= n_valid_examples:\n",
        "          break\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        # Get the index of the max log-probability.\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "      \n",
        "    accuracy = correct / min(len(valid_loader.dataset), n_valid_examples)\n",
        "    \n",
        "    # report back to Optuna how far it is (epoch-wise) into the trial and how well it is doing (accuracy)\n",
        "    trial.report(accuracy, epoch)\n",
        "    # then, Optuna can decide if the trial should be pruned\n",
        "    # Handle pruning based on the intermediate value.\n",
        "    if trial.should_prune():\n",
        "      raise optuna.exceptions.TrialPruned()\n",
        "      \n",
        "  return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfeAXbSkLY1l",
        "outputId": "1823ebf7-42b6-493f-9040-48c5ddea8db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2023-01-15 15:42:11,835]\u001b[0m A new study created in memory with name: resnet-fc\u001b[0m\n",
            "\u001b[32m[I 2023-01-15 17:10:48,941]\u001b[0m Trial 0 finished with value: 0.0328125 and parameters: {'lr': 1.9990262624334293e-06, 'optimizer': 'SGD'}. Best is trial 0 with value: 0.0328125.\u001b[0m\n",
            "\u001b[32m[I 2023-01-15 18:39:35,425]\u001b[0m Trial 1 finished with value: 0.0421875 and parameters: {'lr': 5.477832478778276e-07, 'optimizer': 'Adam'}. Best is trial 1 with value: 0.0421875.\u001b[0m\n",
            "\u001b[32m[I 2023-01-15 20:07:49,938]\u001b[0m Trial 2 finished with value: 0.0421875 and parameters: {'lr': 2.1499185506784375e-06, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.0421875.\u001b[0m\n",
            "\u001b[32m[I 2023-01-15 21:37:32,996]\u001b[0m Trial 3 finished with value: 0.0421875 and parameters: {'lr': 3.846162088975758e-07, 'optimizer': 'RMSprop'}. Best is trial 1 with value: 0.0421875.\u001b[0m\n",
            "\u001b[32m[I 2023-01-15 23:07:15,119]\u001b[0m Trial 4 finished with value: 0.0421875 and parameters: {'lr': 2.6204922821359317e-06, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.0421875.\u001b[0m\n",
            "\u001b[32m[I 2023-01-16 00:37:12,734]\u001b[0m Trial 5 finished with value: 0.0421875 and parameters: {'lr': 4.4684215284583897e-07, 'optimizer': 'SGD'}. Best is trial 1 with value: 0.0421875.\u001b[0m\n",
            "\u001b[32m[I 2023-01-16 00:46:12,780]\u001b[0m Trial 6 pruned. \u001b[0m\n",
            "\u001b[32m[I 2023-01-16 00:55:11,223]\u001b[0m Trial 7 pruned. \u001b[0m\n",
            "\u001b[32m[I 2023-01-16 01:04:11,941]\u001b[0m Trial 8 pruned. \u001b[0m\n",
            "\u001b[32m[I 2023-01-16 01:13:12,721]\u001b[0m Trial 9 pruned. \u001b[0m\n",
            "\u001b[32m[I 2023-01-16 01:22:20,660]\u001b[0m Trial 10 pruned. \u001b[0m\n",
            "\u001b[32m[I 2023-01-16 01:31:18,180]\u001b[0m Trial 11 pruned. \u001b[0m\n",
            "\u001b[32m[I 2023-01-16 01:40:20,239]\u001b[0m Trial 12 pruned. \u001b[0m\n",
            "\u001b[32m[I 2023-01-16 01:49:26,937]\u001b[0m Trial 13 pruned. \u001b[0m\n",
            "\u001b[32m[I 2023-01-16 01:58:24,632]\u001b[0m Trial 14 pruned. \u001b[0m\n",
            "\u001b[32m[I 2023-01-16 02:07:22,670]\u001b[0m Trial 15 pruned. \u001b[0m\n",
            "\u001b[32m[I 2023-01-16 02:16:24,601]\u001b[0m Trial 16 pruned. \u001b[0m\n",
            "\u001b[32m[I 2023-01-16 03:46:52,991]\u001b[0m Trial 17 finished with value: 0.04375 and parameters: {'lr': 1.2467078289502118e-06, 'optimizer': 'Adam'}. Best is trial 17 with value: 0.04375.\u001b[0m\n",
            "\u001b[32m[I 2023-01-16 05:17:12,367]\u001b[0m Trial 18 finished with value: 0.046875 and parameters: {'lr': 1.0194986974126253e-06, 'optimizer': 'Adam'}. Best is trial 18 with value: 0.046875.\u001b[0m\n",
            "\u001b[32m[I 2023-01-16 06:47:30,188]\u001b[0m Trial 19 finished with value: 0.046875 and parameters: {'lr': 1.1516664364191993e-06, 'optimizer': 'Adam'}. Best is trial 18 with value: 0.046875.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Study statistics: \n",
            " Number of finished trials:  20\n",
            " Number of pruned trials:  11\n",
            " Number of complete trials:  9\n",
            "Best trial:\n",
            " Value:  0.046875\n",
            " Params: \n",
            " lr: 1.0194986974126253e-06\n",
            " optimizer: Adam\n"
          ]
        }
      ],
      "source": [
        "# now we can run the experiment\n",
        "sampler = optuna.samplers.TPESampler()\n",
        "study = optuna.create_study(study_name=\"resnet-fc\", direction=\"maximize\", sampler=sampler)\n",
        "study.optimize(objective, n_trials=20)\n",
        "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
        "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
        "print(\"Study statistics: \")\n",
        "print(\" Number of finished trials: \", len(study.trials))\n",
        "print(\" Number of pruned trials: \", len(pruned_trials))\n",
        "print(\" Number of complete trials: \", len(complete_trials))\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(\" Value: \", trial.value)\n",
        "print(\" Params: \")\n",
        "\n",
        "for key, value in trial.params.items():\n",
        " print(\" {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "K_m-rNNalSDI",
        "outputId": "16582986-415d-447b-f295-669ef0ed9662"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ec7b157c78f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_param_importances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'optuna' is not defined"
          ]
        }
      ],
      "source": [
        "optuna.visualization.plot_param_importances(study)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iwsTjYG9rxm"
      },
      "outputs": [],
      "source": [
        "optuna.visualization.plot_contour(study, params=[\"n_units_l0\", \"dropout_l0\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iHALzYV22O3S"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}