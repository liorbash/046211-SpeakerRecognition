# -*- coding: utf-8 -*-
"""data_loader.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F570vUaQ3oQ0rC2hnQHhIzNkC2q4SpH1
"""

from torchaudio import datasets
import torchaudio.transforms
import torchvision.transforms
from torch.utils.data import DataLoader, Dataset
from torch.utils.data.sampler import SubsetRandomSampler
import numpy as np
import torch
from sklearn.preprocessing import MinMaxScaler

import Melspectogram


class VoxCaleb1MelSpecDataset(Dataset):
    def __init__(self, data_dir, subset, transform=None, min_max_scale=True, normalize_mel=True, sr=16000, download=False,):
        """
        Args:
            data_dir (string): Directory with all the audio files.
            subset (string): 'train', 'val'/'dev' or 'test'.
            transform (torch.transforms): Input's transforms.
            min_max_scale (bool): If true, scaling min-max the input after transforms.
            normalize_mel (bool): If true, normalize mel-spectogram by mean and std.
            sr (int): Audio sample rate.
            download (bool): Whether to download the dataset or not.
        """
        self.data_dir = data_dir
        self.transform = transform
        self.min_max_scale = min_max_scale
        self.normalize_mel = normalize_mel
        if subset == 'val':
            subset = 'dev'
        self.dataset = datasets.VoxCeleb1Identification(root=self.data_dir, subset=subset, download=download)
        self.idx = {'waveform': 0, 'sr': 1, 'speaker_id': 2, 'path_wav': 3}
        return

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        sample = self.dataset[idx]

        # Transforms
        sample = list(sample)
        if self.normalize_mel:
            sample[self.idx['waveform']] = Melspectogram.transform(sample[self.idx['waveform']].cpu().numpy()[0])

        if self.transform:
            sample[self.idx['waveform']] = self.transform(sample[self.idx['waveform']])

        if self.min_max_scale:
            scaler = MinMaxScaler()
            sample[self.idx['waveform']][0, :, :] = torch.Tensor(scaler.fit_transform(sample[self.idx['waveform']][0, :, :]))

        sample = tuple(sample)

        # RGB
        image = torch.cat((sample[self.idx['waveform']], sample[self.idx['waveform']], sample[self.idx['waveform']]), 0)
        # labels start at 0 instead of 1
        label = sample[self.idx['speaker_id']] - 1
        return image, label


def get_datasets(input_size, dataset_dir='.', download=False, min_max_scale=False, normalize_mel=True):
    if normalize_mel:
        transforms = torchvision.transforms.Resize((input_size, input_size))
    else:
        transforms = torch.nn.Sequential(
            torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=512, win_length=400, hop_length=160,
                                                 f_min=0.0, f_max=8000, pad=0, n_mels=40),
            torchvision.transforms.Resize((input_size, input_size))
        )
    train_dataset = VoxCaleb1MelSpecDataset(dataset_dir, 'train', transform=transforms, download=download,
                                            min_max_scale=min_max_scale, normalize_mel=normalize_mel)
    validation_dataset = VoxCaleb1MelSpecDataset(dataset_dir, 'dev', transform=transforms, download=download,
                                                 min_max_scale=min_max_scale, normalize_mel=normalize_mel)
    test_dataset = VoxCaleb1MelSpecDataset(dataset_dir, 'test', transform=transforms, download=download,
                                           min_max_scale=min_max_scale, normalize_mel=normalize_mel)
    return {'train': train_dataset, 'val': validation_dataset, 'test': test_dataset}


def get_dataloader(dataset, batch_size, num_workers=2, shuffle=True):
    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,
                                              num_workers=num_workers, shuffle=shuffle)

    return data_loader


def get_dataloaders(dataset_list, train_batch_size, num_workers=2, shuffle=True):
    train_dataloader = get_dataloader(dataset_list['train'], train_batch_size, num_workers=num_workers,
                                      shuffle=shuffle)
    validation_dataloader = get_dataloader(dataset_list['val'], len(dataset_list['val']), num_workers=num_workers,
                                           shuffle=shuffle)
    test_dataloader = get_dataloader(dataset_list['test'], len(dataset_list['test']), num_workers=num_workers,
                                     shuffle=shuffle)
    return {'train': train_dataloader, 'val': validation_dataloader, 'test': test_dataloader}
